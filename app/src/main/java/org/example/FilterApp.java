package org.example;

import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.config.SslConfigs;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;

import data.gen.avro.logistics; // This class is automatically generated by the Avro plugin

import java.util.Collections;
import java.util.Map;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;

public class FilterApp {

    private static final Logger log = LoggerFactory.getLogger(FilterApp.class);

    private static final String INPUT_TOPIC = "logistics_data_gen";
    private static final String OUTPUT_TOPIC = "logistics_data_filtered";

    private static Properties setConfig() {
        // Gather our `-D` arguments
        // There's a sensible default for the schema registry user, so provide it
        String kafkaServiceUri = System.getProperty("KAFKA_SERVICE_URI");
        String sslTruststoreLocation = System.getProperty("SSL_TRUSTSTORE_LOCATION");
        String sslKeystoreLocation = System.getProperty("SSL_KEYSTORE_LOCATION");
        String passwordForStore = System.getProperty("PASSWORD_FOR_STORE");
        String schemaRegistryUrl = System.getProperty("SCHEMA_REGISTRY_URL");
        String schemaRegistryUser = System.getProperty("SCHEMA_REGISTRY_USER", "avnadmin");
        String schemaRegistryPassword = System.getProperty("SCHEMA_REGISTRY_PASSWORD");

        if (kafkaServiceUri == null||
                sslTruststoreLocation == null ||
                sslKeystoreLocation == null ||
                passwordForStore == null ||
                schemaRegistryUrl == null ||
                schemaRegistryUser == null ||
                schemaRegistryPassword == null) {
            if (kafkaServiceUri == null) log.error("Missing value for -DKAFKA_SERVICE_URI");
            if (sslTruststoreLocation == null) log.error("Missing value for -DSSL_TRUSTSTORE_LOCATION");
            if (sslKeystoreLocation == null) log.error("Missing value for -DSSL_KEYSTORE_LOCATION");
            if (passwordForStore == null) log.error("Missing value for -DPASSWORD_FOR_STORE");
            if (schemaRegistryUrl == null) log.error("Missing value for -DSCHEMA_REGISTRY_URL");
            if (schemaRegistryUser == null) log.error("Missing value for -DSCHEMA_REGISTRY_USER");
            if (schemaRegistryPassword == null) log.error("Missing value for -DSCHEMA_REGISTRY_PASSWORD");
            System.exit(1);
        }

        Properties config = new Properties();
        config.put(StreamsConfig.APPLICATION_ID_CONFIG, "json-filter-application");
        config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServiceUri);

        // We're not particularly interested in the message key, so leave it as a string
        // For the message value, we want a *specific* AvroSerde, since we're going to generate
        // a class based on the message schema.
        config.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        config.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, SpecificAvroSerde.class);

        // Security settings.
        // a. These settings must match the security settings of the secure Kafka cluster.
        // b. The SSL trust store and key store files must be locally accessible to the application.
        config.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
        config.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);
        config.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, passwordForStore);
        config.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);
        config.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, passwordForStore);
        config.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, passwordForStore);
        // Schema registry
        config.put("schema.registry.url", schemaRegistryUrl);
        config.put("schema.registry.basic.auth.credentials.source", "USER_INFO");
        config.put("schema.registry.basic.auth.user.info", schemaRegistryUser + ":" + schemaRegistryPassword);

        // For a demo app, let's start at the beginningof the input topic
        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        return config;
    }

    public static void main(String[] args) {
        Properties config = setConfig();

        // Use the Avro schema to define a specific Serdes to that schema
        // We allow it to default to the schema registry we specified earlier
        final SpecificAvroSerde<logistics> logisticsAvroSerde = new SpecificAvroSerde<>();
        // Do I need to configure it explicitly? I hope not - but let's try for now
        String schemaRegistryUrl = System.getProperty("SCHEMA_REGISTRY_URL");
        Map<String, String> serdeConfig = Collections.singletonMap("schema.registry.url", schemaRegistryUrl);
        logisticsAvroSerde.configure(serdeConfig, false);  // Tell it it's for handling values, not keys

        // Define the stream topology
        final StreamsBuilder builder = new StreamsBuilder();

        final Topology topology = builder.build();
        System.out.println(topology.describe());

        // We want a stream that reads from the input topic
        //final KStream<String, String> sourceStream = builder.stream(INPUT_TOPIC);

        // KStream consumed with String key and Greeting (Avro) value
        KStream<String, logistics> inputStream = builder.stream(
            INPUT_TOPIC,
            Consumed.with(Serdes.String(), logisticsAvroSerde)
        );

        // Filter: only keep greetings where the 'name' field is NOT "Sauron"
        // Note we're producing messages with the same schema as we consumer
        KStream<String, logistics> filteredStream = inputStream
            .filter((key, value) -> {
                if (value == null) {
                    // Handle null or tombstone records gracefully
                    return false;
                }
                // Check the 'name' field from the generated Avro object
                boolean shouldKeep = !value.getState().toString().equals("Received");
                System.out.printf("Processing record: Key=%s, State=%s, Keep=%s\n", key, value.getState(), shouldKeep);
                return shouldKeep;
            });

        // Produce the filtered stream to the output topic
        filteredStream.to(
            OUTPUT_TOPIC,
            Produced.with(Serdes.String(), logisticsAvroSerde)
        );

        KafkaStreams streams = new KafkaStreams(builder.build(), config);

        // Add a shutdown hook to close the Streams application gracefully
        final CountDownLatch latch = new CountDownLatch(1);
        Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
            @Override
            public void run() {
                streams.close();
                latch.countDown();
            }
        });

        try {
            // Clean up local state stores (useful for development/testing)
            streams.cleanUp();
            streams.start();
            latch.await();
        } catch (Throwable e) {
            log.error("Error starting or running Kafka Streams application", e);
            System.exit(1);
        }
        log.info("Kafka Streams Application Shut Down.");
        System.exit(0);
    }
}
